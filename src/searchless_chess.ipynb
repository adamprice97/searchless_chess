{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BcWXEQI7Ws2l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import chess\n",
        "import chess.svg\n",
        "from jax import random as jrandom\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oUuSZBYyWvbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 4 rows from ['<demo>']\n",
            "Raw dataset preview (fen, move)\n",
            "                                                 fen   move __source_file__\n",
            "0  rnbqkbnr/pppppppp/8/8/4P3/5N2/PPPP1PPP/RNBQKB1...   d7d5          <demo>\n",
            "1  rnbqkbnr/pppp1ppp/8/4p3/3PP3/5N2/PPP2PPP/RNBQK...   e5d4          <demo>\n",
            "2  rnbqkbnr/ppp2ppp/3p4/4p3/3PP3/4BN2/PPP2PPP/RN1...   e5d4          <demo>\n",
            "3                       8/P7/8/8/8/8/8/k6K w - - 0 1  a7a8q          <demo>\n",
            "Round-trip check (uci -> params -> uci): 100.0% of rows match\n",
            "\n",
            "Top 10 from squares:\n",
            "  36: 2\n",
            "  48: 1\n",
            "  51: 1\n",
            "\n",
            "Top 10 to squares:\n",
            "  27: 2\n",
            "  56: 1\n",
            "  35: 1\n",
            "\n",
            "Top 10 promo ids (0=None,1=q,2=r,3=b,4=n):\n",
            "  0: 3\n",
            "  1: 1\n",
            "\n",
            "Demo forward shape [B,3,64]: (4, 3, 64)\n",
            "Sanity check (per-step log-prob sums should be ~0): True\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/mnt/data/dataset_parametrised_preview.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-8-9c75e5306964>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;31m# -------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[0mout_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/mnt/data/dataset_parametrised_preview.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m \u001b[0mdf_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nSaved a CSV preview with parameterised targets to: {out_path}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3385\u001b[0m         )\n\u001b[0;32m   3386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3387\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/dataset_parametrised_preview.csv'"
          ]
        }
      ],
      "source": [
        "# Jupyter-style exploration utilities for the parameterised BC pipeline.\n",
        "# This notebook code will:\n",
        "# 1) Inspect / load a (fen, move) dataset if present (CSV or JSONL), or make a tiny demo set.\n",
        "# 2) Convert moves into (from_idx, to_idx, promo_id).\n",
        "# 3) (Optionally) run a forward pass through a minimal param-action head if JAX/Haiku are available.\n",
        "#\n",
        "# You can re-run cells as you like.\n",
        "\n",
        "\n",
        "import os, sys, json, csv, math, textwrap, pathlib, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Helpers: UCI <-> params\n",
        "# ----------------------------\n",
        "FILES = \"abcdefgh\"\n",
        "RANKS = \"12345678\"\n",
        "PROMO_TO_ID = {\"\":0, \"q\":1, \"r\":2, \"b\":3, \"n\":4}\n",
        "ID_TO_PROMO = {v:k for k,v in PROMO_TO_ID.items()}\n",
        "\n",
        "def square_to_index(file_char: str, rank_char: str) -> int:\n",
        "    f = FILES.index(file_char)\n",
        "    r = int(rank_char) - 1\n",
        "    return r * 8 + f\n",
        "\n",
        "def index_to_square(idx: int) -> str:\n",
        "    f = idx % 8\n",
        "    r = idx // 8\n",
        "    return f\"{FILES[f]}{RANKS[r]}\"\n",
        "\n",
        "def uci_to_params(uci: str) -> Tuple[int, int, int]:\n",
        "    uci = uci.strip().lower()\n",
        "    if len(uci) < 4:\n",
        "        raise ValueError(f\"Bad UCI: {uci}\")\n",
        "    f_file, f_rank, t_file, t_rank = uci[0], uci[1], uci[2], uci[3]\n",
        "    from_idx = square_to_index(f_file, f_rank)\n",
        "    to_idx = square_to_index(t_file, t_rank)\n",
        "    promo = \"\"\n",
        "    if len(uci) == 5:\n",
        "        promo = uci[4]\n",
        "    return from_idx, to_idx, PROMO_TO_ID.get(promo, 0)\n",
        "\n",
        "def params_to_uci(from_idx: int, to_idx: int, promo_id: int) -> str:\n",
        "    return f\"{index_to_square(from_idx)}{index_to_square(to_idx)}{ID_TO_PROMO.get(promo_id,'')}\"\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Load dataset (try common locations or make a tiny sample)\n",
        "# ------------------------------------------------------------\n",
        "def try_load_dataset() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Try a few common file names:\n",
        "      - /mnt/data/behavioral_cloning.csv\n",
        "      - /mnt/data/behavioral_cloning.jsonl\n",
        "      - ../data/behavioral_cloning.csv (relative to current)\n",
        "    The file format should have columns/fields: 'fen' and 'move' (UCI).\n",
        "    If nothing is found, we create a tiny demo DataFrame.\n",
        "    \"\"\"\n",
        "    candidates = [\n",
        "        \"/mnt/data/behavioral_cloning.csv\",\n",
        "        \"/mnt/data/behavioral_cloning.jsonl\",\n",
        "        \"/mnt/data/data.csv\",\n",
        "        \"/mnt/data/data.jsonl\",\n",
        "        \"../data/behavioral_cloning.csv\",\n",
        "        \"../data/behavioral_cloning.jsonl\",\n",
        "    ]\n",
        "    for path in candidates:\n",
        "        if os.path.exists(path):\n",
        "            if path.endswith(\".csv\"):\n",
        "                df = pd.read_csv(path)\n",
        "            else:\n",
        "                # JSONL\n",
        "                rows = []\n",
        "                with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    for line in f:\n",
        "                        rows.append(json.loads(line))\n",
        "                df = pd.DataFrame(rows)\n",
        "            # normalize cols\n",
        "            if \"uci\" in df.columns and \"move\" not in df.columns:\n",
        "                df = df.rename(columns={\"uci\":\"move\"})\n",
        "            if not {\"fen\",\"move\"}.issubset(df.columns):\n",
        "                raise ValueError(f\"Found {path} but it lacks 'fen'/'move' columns. Columns = {df.columns.tolist()}\")\n",
        "            df[\"__source_file__\"] = path\n",
        "            return df\n",
        "\n",
        "    # Fallback tiny demo set (3 legal positions with a couple of moves)\n",
        "    demo = [\n",
        "        # Scholars mate ideas etc.\n",
        "        {\"fen\":\"rnbqkbnr/pppppppp/8/8/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 1\", \"move\":\"d7d5\"},\n",
        "        {\"fen\":\"rnbqkbnr/pppp1ppp/8/4p3/3PP3/5N2/PPP2PPP/RNBQKB1R b KQkq - 0 2\", \"move\":\"e5d4\"},\n",
        "        {\"fen\":\"rnbqkbnr/ppp2ppp/3p4/4p3/3PP3/4BN2/PPP2PPP/RN1QKB1R b KQkq - 0 3\", \"move\":\"e5d4\"},\n",
        "        # Promotion example\n",
        "        {\"fen\":\"8/P7/8/8/8/8/8/k6K w - - 0 1\", \"move\":\"a7a8q\"},\n",
        "    ]\n",
        "    df = pd.DataFrame(demo)\n",
        "    df[\"__source_file__\"] = \"<demo>\"\n",
        "    return df\n",
        "\n",
        "\n",
        "df = try_load_dataset()\n",
        "print(f\"Loaded {len(df)} rows from {df['__source_file__'].unique().tolist()}\")\n",
        "print(\"Raw dataset preview (fen, move)\")\n",
        "print(df.head(20))\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2) Apply parameterised transform: add from/to/promo columns & checks\n",
        "# ----------------------------------------------------------------------\n",
        "def add_param_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    from_list, to_list, promo_list = [], [], []\n",
        "    for uci in df[\"move\"].astype(str).tolist():\n",
        "        f_idx, t_idx, p_id = uci_to_params(uci)\n",
        "        from_list.append(f_idx)\n",
        "        to_list.append(t_idx)\n",
        "        promo_list.append(p_id)\n",
        "    out = df.copy()\n",
        "    out[\"from_idx\"] = from_list\n",
        "    out[\"to_idx\"] = to_list\n",
        "    out[\"promo_id\"] = promo_list\n",
        "    out[\"reconstructed_uci\"] = [params_to_uci(f,t,p) for f,t,p in zip(from_list, to_list, promo_list)]\n",
        "    out[\"matches_roundtrip\"] = (out[\"reconstructed_uci\"] == out[\"move\"].str.lower())\n",
        "    return out\n",
        "\n",
        "df_params = add_param_columns(df)\n",
        "df_params.head(50)\n",
        "\n",
        "print(\"Round-trip check (uci -> params -> uci):\",\n",
        "      f\"{df_params['matches_roundtrip'].mean()*100:.1f}% of rows match\")\n",
        "\n",
        "# Quick histograms (counts only, textual to keep deps minimal)\n",
        "def quick_counts(series: pd.Series, topk: int = 10, name: str = \"\"):\n",
        "    counts = series.value_counts().head(topk)\n",
        "    print(f\"\\nTop {topk} {name}:\")\n",
        "    for k, v in counts.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "quick_counts(df_params[\"from_idx\"], name=\"from squares\")\n",
        "quick_counts(df_params[\"to_idx\"], name=\"to squares\")\n",
        "quick_counts(df_params[\"promo_id\"], name=\"promo ids (0=None,1=q,2=r,3=b,4=n)\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 3) (Optional) Forward pass demo with a tiny NumPy MLP as a stand-in head\n",
        "# -------------------------------------------------------------------------\n",
        "# We avoid importing JAX/Haiku here (may not be present). This is a sanity\n",
        "# demonstration: given a \"core\" vector and one-hot(from/to), produce logits\n",
        "# for the last three steps shaped [B, 3, 64], with -1e9 padding for promo.\n",
        "#\n",
        "# This is NOT your training model; it's a structure/shape demo you can adapt.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def tiny_mlp(x, out):\n",
        "    # single hidden layer for demo\n",
        "    h = np.tanh(x @ np.random.randn(x.shape[-1], x.shape[-1]) * 0.1)\n",
        "    return h @ np.random.randn(h.shape[-1], out) * 0.1\n",
        "\n",
        "def make_demo_forward(batch_from: np.ndarray, batch_to: np.ndarray, core_dim: int = 64):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      batch_from: [B] ints in [0,63]\n",
        "      batch_to:   [B] ints in [0,63]\n",
        "    Returns:\n",
        "      log_probs: [B, 3, 64]; last step has first 5 classes valid, others ~-1e9\n",
        "    \"\"\"\n",
        "    B = batch_from.shape[0]\n",
        "    V = 64\n",
        "    # Random \"core\" vectors just for a demo\n",
        "    core = np.random.randn(B, core_dim).astype(np.float32)\n",
        "\n",
        "    from_oh = np.eye(64, dtype=np.float32)[batch_from]           # [B,64]\n",
        "    to_oh   = np.eye(64, dtype=np.float32)[batch_to]             # [B,64]\n",
        "\n",
        "    # Head 1: from logits\n",
        "    h1 = core\n",
        "    logits_from = tiny_mlp(h1, V)\n",
        "\n",
        "    # Head 2: to logits (condition on from)\n",
        "    h2 = np.concatenate([core, from_oh], axis=-1)\n",
        "    logits_to = tiny_mlp(h2, V)\n",
        "\n",
        "    # Head 3: promo (condition on from & to)\n",
        "    h3 = np.concatenate([core, from_oh, to_oh], axis=-1)\n",
        "    logits_promo5 = tiny_mlp(h3, 5)\n",
        "    neg_inf = np.full((B, V-5), -1e9, dtype=np.float32)\n",
        "    logits_promo = np.concatenate([logits_promo5, neg_inf], axis=-1)\n",
        "\n",
        "    # Pack to [B, T=3, V=64] and log-softmax\n",
        "    logits = np.zeros((B, 3, V), dtype=np.float32)\n",
        "    logits[:, 0, :] = logits_from\n",
        "    logits[:, 1, :] = logits_to\n",
        "    logits[:, 2, :] = logits_promo\n",
        "\n",
        "    # log-softmax per step\n",
        "    def log_softmax(x, axis=-1):\n",
        "        x_max = np.max(x, axis=axis, keepdims=True)\n",
        "        y = x - x_max\n",
        "        np.exp(y, out=y)\n",
        "        y_sum = np.sum(y, axis=axis, keepdims=True)\n",
        "        return (x - x_max) - np.log(y_sum)\n",
        "\n",
        "    logp = log_softmax(logits, axis=-1)\n",
        "    return logp  # [B,3,64]\n",
        "\n",
        "# Demo forward on the first few rows\n",
        "B = min(8, len(df_params))\n",
        "batch_from = df_params[\"from_idx\"].values[:B].astype(np.int64)\n",
        "batch_to = df_params[\"to_idx\"].values[:B].astype(np.int64)\n",
        "logp_demo = make_demo_forward(batch_from, batch_to, core_dim=64)\n",
        "print(\"\\nDemo forward shape [B,3,64]:\", logp_demo.shape)\n",
        "print(\"Sanity check (per-step log-prob sums should be ~0):\",\n",
        "      np.allclose(np.log(np.sum(np.exp(logp_demo), axis=-1)), 0.0, atol=1e-5))\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 4) Save an inspection CSV with the parameterised columns\n",
        "# -------------------------------------------------------------------------\n",
        "out_path = \"/mnt/data/dataset_parametrised_preview.csv\"\n",
        "df_params.to_csv(out_path, index=False)\n",
        "print(f\"\\nSaved a CSV preview with parameterised targets to: {out_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX backend: cpu\n",
            "Devices: [CpuDevice(id=0)]\n",
            "Result device: TFRT_CPU_0\n"
          ]
        }
      ],
      "source": [
        "import jax, jax.numpy as jnp\n",
        "print(\"JAX backend:\", jax.default_backend())\n",
        "print(\"Devices:\", jax.devices())\n",
        "\n",
        "x = jnp.ones((1024, 1024))\n",
        "y = x @ x\n",
        "print(\"Result device:\", y.device_buffer.device())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//third_party/deepmind/searchless_chess/src:searchless_chess",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
